{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial to build a AlexNet and train on CIFAR\n",
    "#### Project 1, Data Science and Machine Learning Bootcamp\n",
    "\n",
    "Some of the following implementation derive from:\n",
    "[1] Official document of pytorch\n",
    "[2] LRN implementation [here](https://zhuanlan.zhihu.com/p/29786939) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructor note\n",
    "When you write your own project in PyTorch, there are a few things to pay attention to. In this tutorial, we will walk through the very basic steps:\n",
    "\n",
    "    - build network\n",
    "    - define loss and optim method\n",
    "    - prepare dataset\n",
    "    - write some function to control the training/validation flow, nested with visualization\n",
    "    - other utilites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network\n",
    "We will create a class called `AlexNet` to wrap the network. It inherits the basic class `nn.Module` in PyTorch. The `__init__` method defines what type each layer belongs to (`Conv2d`, `ReLU`, etc.) and their configurations.\n",
    "\n",
    "For this step, it is very important to have a graph in sketch first and compute in advance what the size of output maps would be. You need to resort to the [documents](http://pytorch.org/docs/stable/nn.html) if necessary.\n",
    "\n",
    "The other important method is `forward`. It defines how the graph flows in a dynamic manner, meaning it creates the graph (and thus gradients) on the fly in each iteration. You can put some breakpoints here to debug your model (use pdb/pycharm, etc.). To use this function, simply write `output = model(input)`, here `model` stands for some instance you create from class `AlexNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    \"\"\"this is the pytorch version\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        # use .view() to transfer shapes from conv layer to linear layer\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # why this operation?\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the weight initialization here\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        import math\n",
    "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        # here weight and bias is the parameters in each layer\n",
    "        m.weight.data.fill_(1)\n",
    "        m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The flollowing network is exactly the same as the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AlexNet_w_LRN(nn.Module):\n",
    "    \"\"\"this is exactly the same as paper goes\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=96, kernel_size=11, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=96, out_channels=256, kernel_size=5, groups=2, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            LRN(local_size=5, alpha=1e-4, beta=0.75, ACROSS_CHANNELS=True)\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=256, out_channels=384, padding=1, kernel_size=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "        # use .view() to transfer shapes from conv layer to linear layer\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Linear(in_features=6*6*256, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Linear(in_features=4096, out_features=4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        \n",
    "        self.layer8 = nn.Linear(in_features=4096, out_features=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n",
    "        x = x.view(-1, 6*6*256)\n",
    "        x = self.layer8(self.layer7(self.layer6(x)))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For now, there is no official implementation in pytorch (as of Nov 2017).\n",
    "# see the pull request here: https://github.com/pytorch/pytorch/issues/653\n",
    "class LRN(nn.Module):\n",
    "    def __init__(self, local_size=1, alpha=1.0, beta=0.75, ACROSS_CHANNELS=False):\n",
    "        super(LRN, self).__init__()\n",
    "        self.ACROSS_CHANNELS = ACROSS_CHANNELS\n",
    "        if self.ACROSS_CHANNELS:\n",
    "            self.average=nn.AvgPool3d(kernel_size=(local_size, 1, 1), \n",
    "                    stride=1,\n",
    "                    padding=(int((local_size-1.0)/2), 0, 0)) \n",
    "        else:\n",
    "            self.average=nn.AvgPool2d(kernel_size=local_size,\n",
    "                    stride=1,\n",
    "                    padding=int((local_size-1.0)/2))\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.ACROSS_CHANNELS:\n",
    "            div = x.pow(2).unsqueeze(1)\n",
    "            div = self.average(div).squeeze(1)\n",
    "            div = div.mul(self.alpha).add(1.0).pow(self.beta) # '1.0' is the bias\n",
    "        else:\n",
    "            div = x.pow(2)\n",
    "            div = self.average(div)\n",
    "            div = div.mul(self.alpha).add(1.0).pow(self.beta)\n",
    "        x = x.div(div)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set all the hyper-params here\n",
    "batch_size = 4\n",
    "model_type = 'pytorch'  # or 'paper'\n",
    "epoch_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset\n",
    "Pytorch provides some popular datasets in `torchvision` package. Here we use CIFAR-10 as an example. It has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "To prepare a dataset, two steps should be considered. First the `transform` list. It will pre-process the input data based on the list (crop, flip, etc.). Second is to wrap the dataset into an iterator called `Dataloader`, it will prepare the dataset in a mini-batch manner, shuffle the dataset and other stuff (skip for now).\n",
    "\n",
    "If you work on your own project in the future, it is definitely worthy looking into the details of the datasets source code [here](https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py), esp. the `__getitem__` method, in order to write customized one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "transform = T.Compose([\n",
    "                T.Resize(40),\n",
    "                T.RandomCrop(32),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),\n",
    "            ])\n",
    "# change to your own root\n",
    "# or simply put \"./data\" here\n",
    "dataset = dset.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "dataset.num_classes = 10\n",
    "dataset.name = 'cifar10'\n",
    "train_loader = data.DataLoader(dataset, batch_size, num_workers=2, shuffle=True)\n",
    "\n",
    "test_set = dset.CIFAR10(root='./data', train=False, transform=transform)\n",
    "test_loader = data.DataLoader(test_set, batch_size, num_workers=2, shuffle=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side code: show some images for fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " frog   cat   car   cat\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAB6CAYAAACvHqiXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+Ky+pSMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f16a4bf93c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "def imshow(img):\n",
    "    # img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "# print labels\n",
    "print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model, loss, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use pytorch alexnet model\n",
      "USE GPU\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "if model_type == 'pytorch':\n",
    "    model = AlexNet(dataset.num_classes)\n",
    "    model_info = 'use pytorch alexnet model'\n",
    "elif model_type == 'paper':\n",
    "    model = AlexNet_w_LRN(dataset.num_classes)\n",
    "    model_info = 'use model in paper with LRN'\n",
    "print(model_info)\n",
    "# init the weights here\n",
    "model.apply(weight_init)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    model = model.cuda()\n",
    "    print('USE GPU')\n",
    "else:\n",
    "    print('USE CPU')\n",
    "\n",
    "# define loss and optim\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: 9.468506\n",
      "Train Epoch: 0 [40/50000 (0%)]\tLoss: 11.231109\n",
      "Train Epoch: 0 [80/50000 (0%)]\tLoss: 11.490110\n",
      "Train Epoch: 0 [120/50000 (0%)]\tLoss: 3.448144\n",
      "Train Epoch: 0 [160/50000 (0%)]\tLoss: 6.562239\n",
      "Train Epoch: 0 [200/50000 (0%)]\tLoss: 3.231769\n",
      "Train Epoch: 0 [240/50000 (0%)]\tLoss: 5.815630\n",
      "Train Epoch: 0 [280/50000 (1%)]\tLoss: 9.819461\n",
      "Train Epoch: 0 [320/50000 (1%)]\tLoss: 6.898457\n",
      "Train Epoch: 0 [360/50000 (1%)]\tLoss: 7.155758\n",
      "Train Epoch: 0 [400/50000 (1%)]\tLoss: 4.602345\n",
      "Train Epoch: 0 [440/50000 (1%)]\tLoss: 8.998138\n",
      "Train Epoch: 0 [480/50000 (1%)]\tLoss: 7.060830\n",
      "Train Epoch: 0 [520/50000 (1%)]\tLoss: 4.952301\n",
      "Train Epoch: 0 [560/50000 (1%)]\tLoss: 5.914195\n",
      "Train Epoch: 0 [600/50000 (1%)]\tLoss: 4.801013\n",
      "Train Epoch: 0 [640/50000 (1%)]\tLoss: 10.202130\n",
      "Train Epoch: 0 [680/50000 (1%)]\tLoss: 8.900969\n",
      "Train Epoch: 0 [720/50000 (1%)]\tLoss: 8.232727\n",
      "Train Epoch: 0 [760/50000 (2%)]\tLoss: 11.581240\n",
      "Train Epoch: 0 [800/50000 (2%)]\tLoss: 4.809340\n",
      "Train Epoch: 0 [840/50000 (2%)]\tLoss: 3.336779\n",
      "Train Epoch: 0 [880/50000 (2%)]\tLoss: 7.880675\n",
      "Train Epoch: 0 [920/50000 (2%)]\tLoss: 5.385642\n",
      "Train Epoch: 0 [960/50000 (2%)]\tLoss: 9.945496\n",
      "Train Epoch: 0 [1000/50000 (2%)]\tLoss: 5.132983\n",
      "Train Epoch: 0 [1040/50000 (2%)]\tLoss: 7.454713\n",
      "Train Epoch: 0 [1080/50000 (2%)]\tLoss: 5.939073\n",
      "Train Epoch: 0 [1120/50000 (2%)]\tLoss: 4.512516\n",
      "Train Epoch: 0 [1160/50000 (2%)]\tLoss: 3.480308\n",
      "Train Epoch: 0 [1200/50000 (2%)]\tLoss: 4.092981\n",
      "Train Epoch: 0 [1240/50000 (2%)]\tLoss: 5.934384\n",
      "Train Epoch: 0 [1280/50000 (3%)]\tLoss: 5.115629\n",
      "Train Epoch: 0 [1320/50000 (3%)]\tLoss: 4.841473\n",
      "Train Epoch: 0 [1360/50000 (3%)]\tLoss: 9.202624\n",
      "Train Epoch: 0 [1400/50000 (3%)]\tLoss: 3.734284\n",
      "Train Epoch: 0 [1440/50000 (3%)]\tLoss: 8.434643\n",
      "Train Epoch: 0 [1480/50000 (3%)]\tLoss: 5.329577\n",
      "Train Epoch: 0 [1520/50000 (3%)]\tLoss: 3.506397\n",
      "Train Epoch: 0 [1560/50000 (3%)]\tLoss: 8.976299\n",
      "Train Epoch: 0 [1600/50000 (3%)]\tLoss: 8.168124\n",
      "Train Epoch: 0 [1640/50000 (3%)]\tLoss: 7.722667\n",
      "Train Epoch: 0 [1680/50000 (3%)]\tLoss: 9.300714\n",
      "Train Epoch: 0 [1720/50000 (3%)]\tLoss: 4.637161\n",
      "Train Epoch: 0 [1760/50000 (4%)]\tLoss: 4.825214\n",
      "Train Epoch: 0 [1800/50000 (4%)]\tLoss: 5.712871\n",
      "Train Epoch: 0 [1840/50000 (4%)]\tLoss: 7.731726\n",
      "Train Epoch: 0 [1880/50000 (4%)]\tLoss: 5.496256\n",
      "Train Epoch: 0 [1920/50000 (4%)]\tLoss: 6.816943\n",
      "Train Epoch: 0 [1960/50000 (4%)]\tLoss: 9.171497\n",
      "Train Epoch: 0 [2000/50000 (4%)]\tLoss: 8.006413\n",
      "Train Epoch: 0 [2040/50000 (4%)]\tLoss: 9.291305\n",
      "Train Epoch: 0 [2080/50000 (4%)]\tLoss: 5.035856\n",
      "Train Epoch: 0 [2120/50000 (4%)]\tLoss: 6.707123\n",
      "Train Epoch: 0 [2160/50000 (4%)]\tLoss: 6.904501\n",
      "Train Epoch: 0 [2200/50000 (4%)]\tLoss: 3.737404\n",
      "Train Epoch: 0 [2240/50000 (4%)]\tLoss: 5.646400\n",
      "Train Epoch: 0 [2280/50000 (5%)]\tLoss: 6.336925\n",
      "Train Epoch: 0 [2320/50000 (5%)]\tLoss: 6.591924\n",
      "Train Epoch: 0 [2360/50000 (5%)]\tLoss: 8.091427\n",
      "Train Epoch: 0 [2400/50000 (5%)]\tLoss: 3.650006\n",
      "Train Epoch: 0 [2440/50000 (5%)]\tLoss: 6.651965\n",
      "Train Epoch: 0 [2480/50000 (5%)]\tLoss: 4.595862\n",
      "Train Epoch: 0 [2520/50000 (5%)]\tLoss: 8.165123\n",
      "Train Epoch: 0 [2560/50000 (5%)]\tLoss: 6.559284\n",
      "Train Epoch: 0 [2600/50000 (5%)]\tLoss: 7.454389\n",
      "Train Epoch: 0 [2640/50000 (5%)]\tLoss: 5.636200\n",
      "Train Epoch: 0 [2680/50000 (5%)]\tLoss: 7.163831\n",
      "Train Epoch: 0 [2720/50000 (5%)]\tLoss: 11.565625\n",
      "Train Epoch: 0 [2760/50000 (6%)]\tLoss: 6.204658\n",
      "Train Epoch: 0 [2800/50000 (6%)]\tLoss: 9.959884\n",
      "Train Epoch: 0 [2840/50000 (6%)]\tLoss: 7.653143\n",
      "Train Epoch: 0 [2880/50000 (6%)]\tLoss: 7.618047\n",
      "Train Epoch: 0 [2920/50000 (6%)]\tLoss: 9.579509\n",
      "Train Epoch: 0 [2960/50000 (6%)]\tLoss: 5.453082\n",
      "Train Epoch: 0 [3000/50000 (6%)]\tLoss: 9.157446\n",
      "Train Epoch: 0 [3040/50000 (6%)]\tLoss: 4.594302\n",
      "Train Epoch: 0 [3080/50000 (6%)]\tLoss: 7.442302\n",
      "Train Epoch: 0 [3120/50000 (6%)]\tLoss: 5.863645\n",
      "Train Epoch: 0 [3160/50000 (6%)]\tLoss: 6.522734\n",
      "Train Epoch: 0 [3200/50000 (6%)]\tLoss: 7.545604\n",
      "Train Epoch: 0 [3240/50000 (6%)]\tLoss: 8.701066\n",
      "Train Epoch: 0 [3280/50000 (7%)]\tLoss: 7.605856\n",
      "Train Epoch: 0 [3320/50000 (7%)]\tLoss: 5.861212\n",
      "Train Epoch: 0 [3360/50000 (7%)]\tLoss: 5.344914\n",
      "Train Epoch: 0 [3400/50000 (7%)]\tLoss: 6.548735\n",
      "Train Epoch: 0 [3440/50000 (7%)]\tLoss: 9.554169\n",
      "Train Epoch: 0 [3480/50000 (7%)]\tLoss: 3.343666\n",
      "Train Epoch: 0 [3520/50000 (7%)]\tLoss: 7.216518\n",
      "Train Epoch: 0 [3560/50000 (7%)]\tLoss: 6.778101\n",
      "Train Epoch: 0 [3600/50000 (7%)]\tLoss: 5.088090\n",
      "Train Epoch: 0 [3640/50000 (7%)]\tLoss: 5.556138\n",
      "Train Epoch: 0 [3680/50000 (7%)]\tLoss: 7.174356\n",
      "Train Epoch: 0 [3720/50000 (7%)]\tLoss: 6.881271\n",
      "Train Epoch: 0 [3760/50000 (8%)]\tLoss: 4.654133\n",
      "Train Epoch: 0 [3800/50000 (8%)]\tLoss: 4.171729\n",
      "Train Epoch: 0 [3840/50000 (8%)]\tLoss: 6.600095\n",
      "Train Epoch: 0 [3880/50000 (8%)]\tLoss: 3.618988\n",
      "Train Epoch: 0 [3920/50000 (8%)]\tLoss: 5.393365\n",
      "Train Epoch: 0 [3960/50000 (8%)]\tLoss: 5.222619\n",
      "Train Epoch: 0 [4000/50000 (8%)]\tLoss: 10.126867\n",
      "Train Epoch: 0 [4040/50000 (8%)]\tLoss: 7.724320\n",
      "Train Epoch: 0 [4080/50000 (8%)]\tLoss: 9.423031\n",
      "Train Epoch: 0 [4120/50000 (8%)]\tLoss: 9.312603\n",
      "Train Epoch: 0 [4160/50000 (8%)]\tLoss: 5.031666\n",
      "Train Epoch: 0 [4200/50000 (8%)]\tLoss: 8.099888\n",
      "Train Epoch: 0 [4240/50000 (8%)]\tLoss: 9.935707\n",
      "Train Epoch: 0 [4280/50000 (9%)]\tLoss: 5.631705\n",
      "Train Epoch: 0 [4320/50000 (9%)]\tLoss: 7.623457\n",
      "Train Epoch: 0 [4360/50000 (9%)]\tLoss: 6.705027\n",
      "Train Epoch: 0 [4400/50000 (9%)]\tLoss: 3.814585\n",
      "Train Epoch: 0 [4440/50000 (9%)]\tLoss: 6.755969\n",
      "Train Epoch: 0 [4480/50000 (9%)]\tLoss: 3.895100\n",
      "Train Epoch: 0 [4520/50000 (9%)]\tLoss: 6.947220\n",
      "Train Epoch: 0 [4560/50000 (9%)]\tLoss: 6.120270\n",
      "Train Epoch: 0 [4600/50000 (9%)]\tLoss: 7.789503\n",
      "Train Epoch: 0 [4640/50000 (9%)]\tLoss: 8.360518\n",
      "Train Epoch: 0 [4680/50000 (9%)]\tLoss: 8.214547\n",
      "Train Epoch: 0 [4720/50000 (9%)]\tLoss: 7.021960\n",
      "Train Epoch: 0 [4760/50000 (10%)]\tLoss: 6.991400\n",
      "Train Epoch: 0 [4800/50000 (10%)]\tLoss: 6.913437\n",
      "Train Epoch: 0 [4840/50000 (10%)]\tLoss: 2.201398\n",
      "Train Epoch: 0 [4880/50000 (10%)]\tLoss: 6.928862\n",
      "Train Epoch: 0 [4920/50000 (10%)]\tLoss: 6.623719\n",
      "Train Epoch: 0 [4960/50000 (10%)]\tLoss: 3.497416\n",
      "Train Epoch: 0 [5000/50000 (10%)]\tLoss: 5.215027\n",
      "Train Epoch: 0 [5040/50000 (10%)]\tLoss: 5.260706\n",
      "Train Epoch: 0 [5080/50000 (10%)]\tLoss: 5.935934\n",
      "Train Epoch: 0 [5120/50000 (10%)]\tLoss: 6.887971\n",
      "Train Epoch: 0 [5160/50000 (10%)]\tLoss: 5.605381\n",
      "Train Epoch: 0 [5200/50000 (10%)]\tLoss: 6.424438\n",
      "Train Epoch: 0 [5240/50000 (10%)]\tLoss: 9.016417\n",
      "Train Epoch: 0 [5280/50000 (11%)]\tLoss: 11.137774\n",
      "Train Epoch: 0 [5320/50000 (11%)]\tLoss: 6.481694\n",
      "Train Epoch: 0 [5360/50000 (11%)]\tLoss: 8.684515\n",
      "Train Epoch: 0 [5400/50000 (11%)]\tLoss: 9.460775\n",
      "Train Epoch: 0 [5440/50000 (11%)]\tLoss: 3.580969\n",
      "Train Epoch: 0 [5480/50000 (11%)]\tLoss: 6.835277\n",
      "Train Epoch: 0 [5520/50000 (11%)]\tLoss: 7.872630\n",
      "Train Epoch: 0 [5560/50000 (11%)]\tLoss: 7.108982\n",
      "Train Epoch: 0 [5600/50000 (11%)]\tLoss: 6.881039\n",
      "Train Epoch: 0 [5640/50000 (11%)]\tLoss: 5.608399\n",
      "Train Epoch: 0 [5680/50000 (11%)]\tLoss: 8.918156\n",
      "Train Epoch: 0 [5720/50000 (11%)]\tLoss: 6.230693\n",
      "Train Epoch: 0 [5760/50000 (12%)]\tLoss: 8.151732\n",
      "Train Epoch: 0 [5800/50000 (12%)]\tLoss: 6.872178\n",
      "Train Epoch: 0 [5840/50000 (12%)]\tLoss: 7.256017\n",
      "Train Epoch: 0 [5880/50000 (12%)]\tLoss: 3.270308\n",
      "Train Epoch: 0 [5920/50000 (12%)]\tLoss: 8.378633\n",
      "Train Epoch: 0 [5960/50000 (12%)]\tLoss: 5.271207\n",
      "Train Epoch: 0 [6000/50000 (12%)]\tLoss: 6.637187\n",
      "Train Epoch: 0 [6040/50000 (12%)]\tLoss: 6.347878\n",
      "Train Epoch: 0 [6080/50000 (12%)]\tLoss: 5.410935\n",
      "Train Epoch: 0 [6120/50000 (12%)]\tLoss: 6.780001\n",
      "Train Epoch: 0 [6160/50000 (12%)]\tLoss: 6.842238\n",
      "Train Epoch: 0 [6200/50000 (12%)]\tLoss: 4.682424\n",
      "Train Epoch: 0 [6240/50000 (12%)]\tLoss: 6.796223\n",
      "Train Epoch: 0 [6280/50000 (13%)]\tLoss: 3.584396\n",
      "Train Epoch: 0 [6320/50000 (13%)]\tLoss: 5.722454\n",
      "Train Epoch: 0 [6360/50000 (13%)]\tLoss: 10.086418\n",
      "Train Epoch: 0 [6400/50000 (13%)]\tLoss: 6.527119\n",
      "Train Epoch: 0 [6440/50000 (13%)]\tLoss: 4.153412\n",
      "Train Epoch: 0 [6480/50000 (13%)]\tLoss: 9.461975\n",
      "Train Epoch: 0 [6520/50000 (13%)]\tLoss: 6.453104\n",
      "Train Epoch: 0 [6560/50000 (13%)]\tLoss: 10.328789\n",
      "Train Epoch: 0 [6600/50000 (13%)]\tLoss: 6.091798\n",
      "Train Epoch: 0 [6640/50000 (13%)]\tLoss: 6.413658\n",
      "Train Epoch: 0 [6680/50000 (13%)]\tLoss: 6.483671\n",
      "Train Epoch: 0 [6720/50000 (13%)]\tLoss: 9.136620\n",
      "Train Epoch: 0 [6760/50000 (14%)]\tLoss: 7.076091\n",
      "Train Epoch: 0 [6800/50000 (14%)]\tLoss: 5.033036\n",
      "Train Epoch: 0 [6840/50000 (14%)]\tLoss: 5.351508\n",
      "Train Epoch: 0 [6880/50000 (14%)]\tLoss: 5.770604\n",
      "Train Epoch: 0 [6920/50000 (14%)]\tLoss: 14.710004\n",
      "Train Epoch: 0 [6960/50000 (14%)]\tLoss: 3.267674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [7000/50000 (14%)]\tLoss: 7.546973\n",
      "Train Epoch: 0 [7040/50000 (14%)]\tLoss: 10.296246\n",
      "Train Epoch: 0 [7080/50000 (14%)]\tLoss: 8.469522\n",
      "Train Epoch: 0 [7120/50000 (14%)]\tLoss: 7.145964\n",
      "Train Epoch: 0 [7160/50000 (14%)]\tLoss: 7.220522\n",
      "Train Epoch: 0 [7200/50000 (14%)]\tLoss: 9.293987\n",
      "Train Epoch: 0 [7240/50000 (14%)]\tLoss: 7.514903\n",
      "Train Epoch: 0 [7280/50000 (15%)]\tLoss: 6.817154\n",
      "Train Epoch: 0 [7320/50000 (15%)]\tLoss: 6.291779\n",
      "Train Epoch: 0 [7360/50000 (15%)]\tLoss: 7.756186\n",
      "Train Epoch: 0 [7400/50000 (15%)]\tLoss: 4.667734\n",
      "Train Epoch: 0 [7440/50000 (15%)]\tLoss: 6.025434\n",
      "Train Epoch: 0 [7480/50000 (15%)]\tLoss: 5.957823\n",
      "Train Epoch: 0 [7520/50000 (15%)]\tLoss: 4.331567\n",
      "Train Epoch: 0 [7560/50000 (15%)]\tLoss: 5.444976\n",
      "Train Epoch: 0 [7600/50000 (15%)]\tLoss: 6.831868\n",
      "Train Epoch: 0 [7640/50000 (15%)]\tLoss: 3.151561\n",
      "Train Epoch: 0 [7680/50000 (15%)]\tLoss: 5.357457\n",
      "Train Epoch: 0 [7720/50000 (15%)]\tLoss: 7.749092\n",
      "Train Epoch: 0 [7760/50000 (16%)]\tLoss: 7.117851\n",
      "Train Epoch: 0 [7800/50000 (16%)]\tLoss: 12.102419\n",
      "Train Epoch: 0 [7840/50000 (16%)]\tLoss: 7.912019\n",
      "Train Epoch: 0 [7880/50000 (16%)]\tLoss: 5.499722\n",
      "Train Epoch: 0 [7920/50000 (16%)]\tLoss: 6.943278\n",
      "Train Epoch: 0 [7960/50000 (16%)]\tLoss: 9.555223\n",
      "Train Epoch: 0 [8000/50000 (16%)]\tLoss: 8.740324\n",
      "Train Epoch: 0 [8040/50000 (16%)]\tLoss: 5.095458\n",
      "Train Epoch: 0 [8080/50000 (16%)]\tLoss: 4.106070\n",
      "Train Epoch: 0 [8120/50000 (16%)]\tLoss: 6.143640\n",
      "Train Epoch: 0 [8160/50000 (16%)]\tLoss: 9.535125\n",
      "Train Epoch: 0 [8200/50000 (16%)]\tLoss: 7.659610\n",
      "Train Epoch: 0 [8240/50000 (16%)]\tLoss: 7.252488\n",
      "Train Epoch: 0 [8280/50000 (17%)]\tLoss: 6.905618\n",
      "Train Epoch: 0 [8320/50000 (17%)]\tLoss: 14.350476\n",
      "Train Epoch: 0 [8360/50000 (17%)]\tLoss: 6.301428\n",
      "Train Epoch: 0 [8400/50000 (17%)]\tLoss: 5.014599\n",
      "Train Epoch: 0 [8440/50000 (17%)]\tLoss: 6.080913\n",
      "Train Epoch: 0 [8480/50000 (17%)]\tLoss: 9.048563\n",
      "Train Epoch: 0 [8520/50000 (17%)]\tLoss: 8.066521\n",
      "Train Epoch: 0 [8560/50000 (17%)]\tLoss: 3.820456\n",
      "Train Epoch: 0 [8600/50000 (17%)]\tLoss: 6.113371\n",
      "Train Epoch: 0 [8640/50000 (17%)]\tLoss: 7.898155\n",
      "Train Epoch: 0 [8680/50000 (17%)]\tLoss: 7.112897\n",
      "Train Epoch: 0 [8720/50000 (17%)]\tLoss: 6.095346\n",
      "Train Epoch: 0 [8760/50000 (18%)]\tLoss: 6.816008\n",
      "Train Epoch: 0 [8800/50000 (18%)]\tLoss: 6.397449\n",
      "Train Epoch: 0 [8840/50000 (18%)]\tLoss: 5.366868\n",
      "Train Epoch: 0 [8880/50000 (18%)]\tLoss: 11.385323\n",
      "Train Epoch: 0 [8920/50000 (18%)]\tLoss: 6.947152\n",
      "Train Epoch: 0 [8960/50000 (18%)]\tLoss: 12.585140\n",
      "Train Epoch: 0 [9000/50000 (18%)]\tLoss: 3.877748\n",
      "Train Epoch: 0 [9040/50000 (18%)]\tLoss: 4.039555\n",
      "Train Epoch: 0 [9080/50000 (18%)]\tLoss: 2.910089\n",
      "Train Epoch: 0 [9120/50000 (18%)]\tLoss: 8.452099\n",
      "Train Epoch: 0 [9160/50000 (18%)]\tLoss: 4.842929\n",
      "Train Epoch: 0 [9200/50000 (18%)]\tLoss: 5.320963\n",
      "Train Epoch: 0 [9240/50000 (18%)]\tLoss: 6.155110\n",
      "Train Epoch: 0 [9280/50000 (19%)]\tLoss: 8.993406\n",
      "Train Epoch: 0 [9320/50000 (19%)]\tLoss: 6.718253\n",
      "Train Epoch: 0 [9360/50000 (19%)]\tLoss: 9.445259\n",
      "Train Epoch: 0 [9400/50000 (19%)]\tLoss: 6.050142\n",
      "Train Epoch: 0 [9440/50000 (19%)]\tLoss: 8.953461\n",
      "Train Epoch: 0 [9480/50000 (19%)]\tLoss: 7.966484\n",
      "Train Epoch: 0 [9520/50000 (19%)]\tLoss: 6.198191\n",
      "Train Epoch: 0 [9560/50000 (19%)]\tLoss: 5.635983\n",
      "Train Epoch: 0 [9600/50000 (19%)]\tLoss: 5.869684\n",
      "Train Epoch: 0 [9640/50000 (19%)]\tLoss: 7.429653\n",
      "Train Epoch: 0 [9680/50000 (19%)]\tLoss: 10.129797\n",
      "Train Epoch: 0 [9720/50000 (19%)]\tLoss: 3.870023\n",
      "Train Epoch: 0 [9760/50000 (20%)]\tLoss: 5.244468\n",
      "Train Epoch: 0 [9800/50000 (20%)]\tLoss: 5.174239\n",
      "Train Epoch: 0 [9840/50000 (20%)]\tLoss: 4.889946\n",
      "Train Epoch: 0 [9880/50000 (20%)]\tLoss: 7.454547\n",
      "Train Epoch: 0 [9920/50000 (20%)]\tLoss: 8.225121\n",
      "Train Epoch: 0 [9960/50000 (20%)]\tLoss: 6.379807\n",
      "Train Epoch: 0 [10000/50000 (20%)]\tLoss: 4.415675\n",
      "Train Epoch: 0 [10040/50000 (20%)]\tLoss: 5.742791\n",
      "Train Epoch: 0 [10080/50000 (20%)]\tLoss: 6.080879\n",
      "Train Epoch: 0 [10120/50000 (20%)]\tLoss: 3.299581\n",
      "Train Epoch: 0 [10160/50000 (20%)]\tLoss: 5.644825\n",
      "Train Epoch: 0 [10200/50000 (20%)]\tLoss: 4.946512\n",
      "Train Epoch: 0 [10240/50000 (20%)]\tLoss: 7.033570\n",
      "Train Epoch: 0 [10280/50000 (21%)]\tLoss: 7.133357\n",
      "Train Epoch: 0 [10320/50000 (21%)]\tLoss: 13.069071\n",
      "Train Epoch: 0 [10360/50000 (21%)]\tLoss: 4.835640\n",
      "Train Epoch: 0 [10400/50000 (21%)]\tLoss: 10.037016\n",
      "Train Epoch: 0 [10440/50000 (21%)]\tLoss: 9.461888\n",
      "Train Epoch: 0 [10480/50000 (21%)]\tLoss: 5.393024\n",
      "Train Epoch: 0 [10520/50000 (21%)]\tLoss: 6.665935\n",
      "Train Epoch: 0 [10560/50000 (21%)]\tLoss: 6.359941\n",
      "Train Epoch: 0 [10600/50000 (21%)]\tLoss: 5.211501\n",
      "Train Epoch: 0 [10640/50000 (21%)]\tLoss: 6.152994\n",
      "Train Epoch: 0 [10680/50000 (21%)]\tLoss: 5.862412\n",
      "Train Epoch: 0 [10720/50000 (21%)]\tLoss: 7.886483\n",
      "Train Epoch: 0 [10760/50000 (22%)]\tLoss: 6.336674\n",
      "Train Epoch: 0 [10800/50000 (22%)]\tLoss: 8.966934\n",
      "Train Epoch: 0 [10840/50000 (22%)]\tLoss: 8.103203\n",
      "Train Epoch: 0 [10880/50000 (22%)]\tLoss: 8.083944\n",
      "Train Epoch: 0 [10920/50000 (22%)]\tLoss: 8.899390\n",
      "Train Epoch: 0 [10960/50000 (22%)]\tLoss: 5.062335\n",
      "Train Epoch: 0 [11000/50000 (22%)]\tLoss: 5.970186\n",
      "Train Epoch: 0 [11040/50000 (22%)]\tLoss: 9.403593\n",
      "Train Epoch: 0 [11080/50000 (22%)]\tLoss: 8.590528\n",
      "Train Epoch: 0 [11120/50000 (22%)]\tLoss: 8.424286\n",
      "Train Epoch: 0 [11160/50000 (22%)]\tLoss: 4.394631\n",
      "Train Epoch: 0 [11200/50000 (22%)]\tLoss: 6.037513\n",
      "Train Epoch: 0 [11240/50000 (22%)]\tLoss: 4.313485\n",
      "Train Epoch: 0 [11280/50000 (23%)]\tLoss: 8.231672\n",
      "Train Epoch: 0 [11320/50000 (23%)]\tLoss: 6.574019\n",
      "Train Epoch: 0 [11360/50000 (23%)]\tLoss: 9.116869\n",
      "Train Epoch: 0 [11400/50000 (23%)]\tLoss: 4.229361\n",
      "Train Epoch: 0 [11440/50000 (23%)]\tLoss: 5.291343\n",
      "Train Epoch: 0 [11480/50000 (23%)]\tLoss: 5.432969\n",
      "Train Epoch: 0 [11520/50000 (23%)]\tLoss: 5.839126\n",
      "Train Epoch: 0 [11560/50000 (23%)]\tLoss: 3.187614\n",
      "Train Epoch: 0 [11600/50000 (23%)]\tLoss: 4.625711\n",
      "Train Epoch: 0 [11640/50000 (23%)]\tLoss: 8.833952\n",
      "Train Epoch: 0 [11680/50000 (23%)]\tLoss: 3.384366\n",
      "Train Epoch: 0 [11720/50000 (23%)]\tLoss: 1.603249\n",
      "Train Epoch: 0 [11760/50000 (24%)]\tLoss: 4.251615\n",
      "Train Epoch: 0 [11800/50000 (24%)]\tLoss: 5.024399\n",
      "Train Epoch: 0 [11840/50000 (24%)]\tLoss: 4.369977\n",
      "Train Epoch: 0 [11880/50000 (24%)]\tLoss: 1.385452\n",
      "Train Epoch: 0 [11920/50000 (24%)]\tLoss: 8.784204\n",
      "Train Epoch: 0 [11960/50000 (24%)]\tLoss: 6.993792\n",
      "Train Epoch: 0 [12000/50000 (24%)]\tLoss: 3.885141\n",
      "Train Epoch: 0 [12040/50000 (24%)]\tLoss: 4.897264\n",
      "Train Epoch: 0 [12080/50000 (24%)]\tLoss: 8.730416\n",
      "Train Epoch: 0 [12120/50000 (24%)]\tLoss: 6.549047\n",
      "Train Epoch: 0 [12160/50000 (24%)]\tLoss: 7.448365\n",
      "Train Epoch: 0 [12200/50000 (24%)]\tLoss: 6.839431\n",
      "Train Epoch: 0 [12240/50000 (24%)]\tLoss: 4.449266\n",
      "Train Epoch: 0 [12280/50000 (25%)]\tLoss: 5.879443\n",
      "Train Epoch: 0 [12320/50000 (25%)]\tLoss: 6.816974\n",
      "Train Epoch: 0 [12360/50000 (25%)]\tLoss: 7.093164\n",
      "Train Epoch: 0 [12400/50000 (25%)]\tLoss: 7.123466\n",
      "Train Epoch: 0 [12440/50000 (25%)]\tLoss: 2.278856\n",
      "Train Epoch: 0 [12480/50000 (25%)]\tLoss: 7.312618\n",
      "Train Epoch: 0 [12520/50000 (25%)]\tLoss: 6.998322\n",
      "Train Epoch: 0 [12560/50000 (25%)]\tLoss: 9.902143\n",
      "Train Epoch: 0 [12600/50000 (25%)]\tLoss: 7.295524\n",
      "Train Epoch: 0 [12640/50000 (25%)]\tLoss: 9.307608\n",
      "Train Epoch: 0 [12680/50000 (25%)]\tLoss: 3.698369\n",
      "Train Epoch: 0 [12720/50000 (25%)]\tLoss: 7.050139\n",
      "Train Epoch: 0 [12760/50000 (26%)]\tLoss: 8.170866\n",
      "Train Epoch: 0 [12800/50000 (26%)]\tLoss: 6.598151\n",
      "Train Epoch: 0 [12840/50000 (26%)]\tLoss: 10.363461\n",
      "Train Epoch: 0 [12880/50000 (26%)]\tLoss: 6.441175\n",
      "Train Epoch: 0 [12920/50000 (26%)]\tLoss: 12.542549\n",
      "Train Epoch: 0 [12960/50000 (26%)]\tLoss: 10.176171\n",
      "Train Epoch: 0 [13000/50000 (26%)]\tLoss: 5.365004\n",
      "Train Epoch: 0 [13040/50000 (26%)]\tLoss: 6.114299\n",
      "Train Epoch: 0 [13080/50000 (26%)]\tLoss: 6.629186\n",
      "Train Epoch: 0 [13120/50000 (26%)]\tLoss: 6.497017\n",
      "Train Epoch: 0 [13160/50000 (26%)]\tLoss: 16.341576\n",
      "Train Epoch: 0 [13200/50000 (26%)]\tLoss: 6.199074\n",
      "Train Epoch: 0 [13240/50000 (26%)]\tLoss: 5.513950\n",
      "Train Epoch: 0 [13280/50000 (27%)]\tLoss: 2.866783\n",
      "Train Epoch: 0 [13320/50000 (27%)]\tLoss: 7.656122\n",
      "Train Epoch: 0 [13360/50000 (27%)]\tLoss: 5.515615\n",
      "Train Epoch: 0 [13400/50000 (27%)]\tLoss: 6.028190\n",
      "Train Epoch: 0 [13440/50000 (27%)]\tLoss: 6.195027\n",
      "Train Epoch: 0 [13480/50000 (27%)]\tLoss: 3.950868\n",
      "Train Epoch: 0 [13520/50000 (27%)]\tLoss: 8.690521\n",
      "Train Epoch: 0 [13560/50000 (27%)]\tLoss: 13.811960\n",
      "Train Epoch: 0 [13600/50000 (27%)]\tLoss: 2.887136\n",
      "Train Epoch: 0 [13640/50000 (27%)]\tLoss: 9.936922\n",
      "Train Epoch: 0 [13680/50000 (27%)]\tLoss: 6.038369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [13720/50000 (27%)]\tLoss: 5.383166\n",
      "Train Epoch: 0 [13760/50000 (28%)]\tLoss: 6.802917\n",
      "Train Epoch: 0 [13800/50000 (28%)]\tLoss: 8.931589\n",
      "Train Epoch: 0 [13840/50000 (28%)]\tLoss: 8.469539\n",
      "Train Epoch: 0 [13880/50000 (28%)]\tLoss: 4.961928\n",
      "Train Epoch: 0 [13920/50000 (28%)]\tLoss: 6.036089\n",
      "Train Epoch: 0 [13960/50000 (28%)]\tLoss: 4.109519\n",
      "Train Epoch: 0 [14000/50000 (28%)]\tLoss: 10.593351\n",
      "Train Epoch: 0 [14040/50000 (28%)]\tLoss: 11.699144\n",
      "Train Epoch: 0 [14080/50000 (28%)]\tLoss: 6.277892\n",
      "Train Epoch: 0 [14120/50000 (28%)]\tLoss: 4.123394\n",
      "Train Epoch: 0 [14160/50000 (28%)]\tLoss: 7.537880\n",
      "Train Epoch: 0 [14200/50000 (28%)]\tLoss: 12.561451\n",
      "Train Epoch: 0 [14240/50000 (28%)]\tLoss: 5.492022\n",
      "Train Epoch: 0 [14280/50000 (29%)]\tLoss: 4.015340\n",
      "Train Epoch: 0 [14320/50000 (29%)]\tLoss: 5.041958\n",
      "Train Epoch: 0 [14360/50000 (29%)]\tLoss: 8.949246\n",
      "Train Epoch: 0 [14400/50000 (29%)]\tLoss: 8.768802\n",
      "Train Epoch: 0 [14440/50000 (29%)]\tLoss: 6.176195\n",
      "Train Epoch: 0 [14480/50000 (29%)]\tLoss: 4.722510\n",
      "Train Epoch: 0 [14520/50000 (29%)]\tLoss: 4.223425\n",
      "Train Epoch: 0 [14560/50000 (29%)]\tLoss: 8.548537\n",
      "Train Epoch: 0 [14600/50000 (29%)]\tLoss: 9.641018\n",
      "Train Epoch: 0 [14640/50000 (29%)]\tLoss: 10.405724\n",
      "Train Epoch: 0 [14680/50000 (29%)]\tLoss: 3.926166\n",
      "Train Epoch: 0 [14720/50000 (29%)]\tLoss: 6.552562\n",
      "Train Epoch: 0 [14760/50000 (30%)]\tLoss: 5.877926\n",
      "Train Epoch: 0 [14800/50000 (30%)]\tLoss: 9.138571\n",
      "Train Epoch: 0 [14840/50000 (30%)]\tLoss: 11.263941\n",
      "Train Epoch: 0 [14880/50000 (30%)]\tLoss: 5.511120\n",
      "Train Epoch: 0 [14920/50000 (30%)]\tLoss: 3.920294\n",
      "Train Epoch: 0 [14960/50000 (30%)]\tLoss: 7.367574\n",
      "Train Epoch: 0 [15000/50000 (30%)]\tLoss: 8.467885\n",
      "Train Epoch: 0 [15040/50000 (30%)]\tLoss: 4.960936\n",
      "Train Epoch: 0 [15080/50000 (30%)]\tLoss: 5.993157\n",
      "Train Epoch: 0 [15120/50000 (30%)]\tLoss: 7.398153\n",
      "Train Epoch: 0 [15160/50000 (30%)]\tLoss: 5.996458\n",
      "Train Epoch: 0 [15200/50000 (30%)]\tLoss: 5.779976\n",
      "Train Epoch: 0 [15240/50000 (30%)]\tLoss: 6.825610\n",
      "Train Epoch: 0 [15280/50000 (31%)]\tLoss: 7.018311\n",
      "Train Epoch: 0 [15320/50000 (31%)]\tLoss: 4.949846\n",
      "Train Epoch: 0 [15360/50000 (31%)]\tLoss: 2.920957\n",
      "Train Epoch: 0 [15400/50000 (31%)]\tLoss: 12.096308\n",
      "Train Epoch: 0 [15440/50000 (31%)]\tLoss: 11.447965\n",
      "Train Epoch: 0 [15480/50000 (31%)]\tLoss: 13.671123\n",
      "Train Epoch: 0 [15520/50000 (31%)]\tLoss: 5.284055\n",
      "Train Epoch: 0 [15560/50000 (31%)]\tLoss: 5.527912\n",
      "Train Epoch: 0 [15600/50000 (31%)]\tLoss: 5.218688\n",
      "Train Epoch: 0 [15640/50000 (31%)]\tLoss: 7.859618\n",
      "Train Epoch: 0 [15680/50000 (31%)]\tLoss: 7.815209\n",
      "Train Epoch: 0 [15720/50000 (31%)]\tLoss: 6.674748\n",
      "Train Epoch: 0 [15760/50000 (32%)]\tLoss: 8.943002\n",
      "Train Epoch: 0 [15800/50000 (32%)]\tLoss: 4.390568\n",
      "Train Epoch: 0 [15840/50000 (32%)]\tLoss: 2.776752\n",
      "Train Epoch: 0 [15880/50000 (32%)]\tLoss: 12.161324\n",
      "Train Epoch: 0 [15920/50000 (32%)]\tLoss: 5.060771\n",
      "Train Epoch: 0 [15960/50000 (32%)]\tLoss: 4.583551\n",
      "Train Epoch: 0 [16000/50000 (32%)]\tLoss: 5.116321\n",
      "Train Epoch: 0 [16040/50000 (32%)]\tLoss: 7.549105\n",
      "Train Epoch: 0 [16080/50000 (32%)]\tLoss: 6.100586\n",
      "Train Epoch: 0 [16120/50000 (32%)]\tLoss: 7.337457\n",
      "Train Epoch: 0 [16160/50000 (32%)]\tLoss: 2.580287\n",
      "Train Epoch: 0 [16200/50000 (32%)]\tLoss: 6.795897\n",
      "Train Epoch: 0 [16240/50000 (32%)]\tLoss: 4.201934\n",
      "Train Epoch: 0 [16280/50000 (33%)]\tLoss: 8.020992\n",
      "Train Epoch: 0 [16320/50000 (33%)]\tLoss: 5.011905\n",
      "Train Epoch: 0 [16360/50000 (33%)]\tLoss: 13.296632\n",
      "Train Epoch: 0 [16400/50000 (33%)]\tLoss: 9.791203\n",
      "Train Epoch: 0 [16440/50000 (33%)]\tLoss: 7.447915\n",
      "Train Epoch: 0 [16480/50000 (33%)]\tLoss: 4.910247\n",
      "Train Epoch: 0 [16520/50000 (33%)]\tLoss: 5.223566\n",
      "Train Epoch: 0 [16560/50000 (33%)]\tLoss: 4.062408\n",
      "Train Epoch: 0 [16600/50000 (33%)]\tLoss: 5.342482\n",
      "Train Epoch: 0 [16640/50000 (33%)]\tLoss: 6.038505\n",
      "Train Epoch: 0 [16680/50000 (33%)]\tLoss: 6.964372\n",
      "Train Epoch: 0 [16720/50000 (33%)]\tLoss: 6.958149\n",
      "Train Epoch: 0 [16760/50000 (34%)]\tLoss: 5.441154\n",
      "Train Epoch: 0 [16800/50000 (34%)]\tLoss: 3.135318\n",
      "Train Epoch: 0 [16840/50000 (34%)]\tLoss: 3.551384\n",
      "Train Epoch: 0 [16880/50000 (34%)]\tLoss: 11.258926\n",
      "Train Epoch: 0 [16920/50000 (34%)]\tLoss: 4.336890\n",
      "Train Epoch: 0 [16960/50000 (34%)]\tLoss: 12.242012\n",
      "Train Epoch: 0 [17000/50000 (34%)]\tLoss: 6.282943\n",
      "Train Epoch: 0 [17040/50000 (34%)]\tLoss: 7.798689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process Process-13:\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 42, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/datasets/cifar.py\", line 122, in __getitem__\n",
      "    img = self.transform(img)\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 42, in __call__\n",
      "    img = t(img)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py\", line 288, in __call__\n",
      "    return F.crop(img, i, j, h, w)\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/functional.py\", line 255, in crop\n",
      "    return img.crop((j, i, j + w, i + h))\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 36, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1038, in crop\n",
      "    return _ImageCrop(self, box)\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 1983, in __init__\n",
      "    self.im = im.im\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/hongyang/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bc92731b6f72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training finished!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-bc92731b6f72>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# step1: clear the gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# step2: run the model and get outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# step3: compute loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                    \u001b[1;31m# step4: compute gradient using loss.backward()\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hongyang/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8eab8a10e887>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# why this operation?\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hongyang/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hongyang/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hongyang/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_gpu:\n",
    "            data, target = data.cuda(), target.cuda()   # put the Tensor on GPU\n",
    "        data, target = Variable(data), Variable(target)\n",
    "         \n",
    "        optimizer.zero_grad()  # step1: clear the gradients\n",
    "        output = model(data)   # step2: run the model and get outputs\n",
    "        loss = criterion(output, target)   # step3: compute loss\n",
    "        loss.backward()                    # step4: compute gradient using loss.backward()\n",
    "        optimizer.step()                   # step5: update the parameter (w - \\alpha * grad_w)\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "            \n",
    "\n",
    "for epoch in range(epoch_size):\n",
    "    train(epoch)\n",
    "print('Training finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a sense of the gpu mode from my terminal:\n",
    "![gpu shown in terminal](https://github.com/hli2020/alexnet-syngrad-pytorch/blob/master/gpu_mode.png)\n",
    "\n",
    "It is really fast! AlexNet is small, only takes around 500 Mb memory (the first gpu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now get your hands dirty\n",
    "\n",
    "What we do above is just the very basic of the alexnet on CIFAR-10. There are defenitely many aspects that you can explore. The very first thing after training is that, test the performance on the test data!\n",
    "\n",
    "Write your own code below. For reference, see [here](http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#test-the-network-on-the-test-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write your own code about testing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try other components\n",
    "Play with model to change other factors to further understand the workflow.\n",
    "\n",
    "    - different learning strategies (adam, rmsprop, etc.)\n",
    "    - bigger batch size really helpful?\n",
    "    - remove FC and replace it with fully convolutional layers\n",
    "    - data augmentation (e.g., multi-crop at test stage)\n",
    "    - other network structures\n",
    "    - compare time in GPU and CPU mode\n",
    "    - other non-linearities\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extensions and experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't have a GPU computer, don't worry. Let's do deep learning on cpu first. In the next project, we will learn to use Google AutoML Cloud, an online service with GPU support."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
