{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Code for JulyOnline - Synthetic gradients\n",
    "\n",
    "This is adapted from [this repo](https://github.com/andrewliao11/dni.pytorch) with some important modifications to make it run.\n",
    "\n",
    "This is the main file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from train import *\n",
    "from dataset import *\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[%] model name will be mnist.mlp_dni.conditioned\n",
      "Epoch [1/300], Step [100/600], Loss: 2.2965, Grad Loss: 0.0150\n",
      "Epoch [1/300], Step [200/600], Loss: 2.2849, Grad Loss: 0.0080\n",
      "Epoch [1/300], Step [300/600], Loss: 2.3024, Grad Loss: 0.0058\n",
      "Epoch [1/300], Step [400/600], Loss: 2.2969, Grad Loss: 0.0034\n",
      "Epoch [1/300], Step [500/600], Loss: 2.3040, Grad Loss: 0.0030\n",
      "Epoch [1/300], Step [600/600], Loss: 2.3031, Grad Loss: 0.0024\n",
      "Epoch [2/300], Step [100/600], Loss: 2.2940, Grad Loss: 0.0018\n",
      "Epoch [2/300], Step [200/600], Loss: 2.2902, Grad Loss: 0.0014\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ed6e22847223>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcifar10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# init the model; in the \"train.py\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m             \u001b[1;31m# the overall structure of the code system; in the \"train.py\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/hongyang/Documents/GitHub/alexnet-syngrad-pytorch/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     97\u001b[0m                 \u001b[1;31m# Forward + Backward + Optimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 loss, grad_loss = self.optimizer_dni_module(images, labels, labels_onehot, \n\u001b[1;32m---> 99\u001b[1;33m                                           self.net.grad_optimizer, self.net.optimizer, self.net)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/hongyang/Documents/GitHub/alexnet-syngrad-pytorch/train.py\u001b[0m in \u001b[0;36moptimizer_dni_module\u001b[1;34m(self, images, labels, label_onehot, grad_optimizer, optimizer, forward)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mgrad_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mgrad_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'grad_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'classify_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='DNI')\n",
    "parser.add_argument('--dataset', choices=['mnist', 'cifar10'], default='mnist')\n",
    "parser.add_argument('--num_epochs', type=int, default=300)\n",
    "parser.add_argument('--model_type', choices=['mlp', 'cnn'], default='mlp', help='currently support mlp and cnn')\n",
    "parser.add_argument('--batch_size', type=int, default=100)\n",
    "parser.add_argument('--conditioned', type=bool, default=True)\n",
    "parser.add_argument('--plot', type=bool, default=True)\n",
    "parser.add_argument('--use_gpu', type=bool, default=True)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# do not support using mlp to trian cifar\n",
    "assert args.dataset != 'cifar10' or args.model_type != 'mlp'\n",
    "model_name = '%s.%s_dni' % (args.dataset, args.model_type, )\n",
    "if args.conditioned:\n",
    "    model_name += '.conditioned'\n",
    "args.model_name = model_name\n",
    "if args.dataset == 'mnist':\n",
    "    data = mnist(args)\n",
    "elif args.dataset == 'cifar10':\n",
    "    data = cifar10(args)\n",
    "m = classifier(args, data)  # init the model; in the \"train.py\"\n",
    "m.train_model()             # the overall structure of the code system; in the \"train.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At the end of the day, it will stop at epoch 300 and the training terminates with model saved the following loss curve saved also. Try it out!\n",
    "\n",
    "The classification loss:\n",
    "\n",
    "![fig1](https://github.com/hli2020/alexnet-syngrad-pytorch/mnist.mlp_dni.conditioned_classify_loss.png \"Classification loss\")\n",
    "\n",
    "The gradient loss (L2 norm):\n",
    "\n",
    "![fig2](https://github.com/hli2020/alexnet-syngrad-pytorch/mnist.mlp_dni.conditioned_grad_loss.png \"Grad loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
